---
title: "Practical Machine Learning Sensor Data"
author: "Marco Marchetti"
date: "16 August 2017"
output:
  html_document:
    keep_md: yes
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE)


```

## Background
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement – a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, the goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways:  
* Class A - exactly according to the specification  
* Class B - throwing the elbows to the front  
* Class C - lifting the dumbbell only halfway  
* Class D - lowering the dumbbell only halfway  
* Class E - throwing the hips to the front  

More information is available from the website here: <http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har> (see the section on the Weight Lifting Exercise Dataset).

## Project Goal
The goal of the  project is to predict the manner in which participant did the exercise ("classe" variable in the training set). We will use prediction model to predict 20 different test cases.

## Data Cleaning and Preparation
We first load the R packages needed for analysis and then download the training and testing data sets.
```{r results="hide"}
library(caret)
library(parallel)
library(doParallel)
library(rattle)
library(e1071)

# Getting Data
trainUrl <-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
trainFile <- "pml-training.csv"
testFile  <- "pml-testing.csv"
if (!file.exists(trainFile)) {
  download.file(trainUrl, destfile=trainFile)
}
if (!file.exists(testFile)) {
  download.file(testUrl, destfile=testFile)
}
trainRaw <- read.csv("pml-training.csv",na.strings=c("NA","#DIV/0!",""))
testRaw <- read.csv("pml-testing.csv",na.strings=c("NA","#DIV/0!",""))
```

### Exploring
We will explore the raw pml training set to detect main information on data. 
```{r}
class(trainRaw$classe)
levels(trainRaw$classe)
dim(trainRaw) # 19622 observations and 160 variables
length(which(as.numeric(colSums(is.na(trainRaw)))!=0)) # how many column with NA
```
The Training set has **19622** observations and **160** variables. There are a lot of missing data, **100** variables contains mainly NA (all greater than 19000 NA)
. The testing data set contains **20** observations.  

### Cleaning
The data have a lot of NA values so we will remove all NA columns.  
We will also remove the first seven predictors since these variables includes log information (user_name, timestamp....) and are not related to the activity measurements.
```{r}
# remove NA columns
trainClean <- trainRaw[,which(as.numeric(colSums(is.na(trainRaw)))==0)]				
testClean <- testRaw[,which(as.numeric(colSums(is.na(testRaw)))==0)]	
# remove unnecessary vars
trainClean <- trainClean[,-(1:7)]				
testClean <- testClean[,-(1:7)]
```
After the cleaning phase the train (trainClean) and the test (testClean) data sets remains with **53** variables.

### Data Partitioning
Since we will be predicting classes in the testing dataset, we will split the training data into a training set (60%) and a validation set (40%).
We will use the training set to train the model and the validation set to assess the model performance.
```{r}
set.seed(1965)
inTrain = createDataPartition(trainClean$classe, p=0.6, list = FALSE)
trainSet = trainClean[inTrain, ]
validationSet = trainClean[-inTrain, ]
```

## Model Building
For this classification problem we will build a predictive model for activity recognition using the Decision Tree approach because it is easy to interpret and works well in a non-linear settings.
We will execute and compare 3 different "trees based" models looking which provides the best out of sample accuracy:  
1.  Classification and Regression trees (rpart)  
2.	Random forest decision trees (rf)  
3.	Stochastic gradient boosting trees (gbm)  

We will execute Cross Validation for the models using K = 5. 
Since we will use all predictor variables and k-fold cross validation the train would be computationally intensive and so we will execute Random Forest and Gradient Boosting with parallel computation.
```{r}
#cross validation
fitControl <- trainControl(method='cv', number = 5)
```

### 1. Classification and Regression trees
We will start to build a predictive model for activity recognition using Classification and Regression trees approach. We will use 5-fold cross validation but in this case we will not use parallel computation.
```{r results="hide"}
modelRpart <- train(classe ~ ., data = trainSet, method = "rpart", trControl = fitControl)
save(modelRpart, file='./modelRpart.RData')
```

```{r}
#Model
modelRpart
#Prediction and confusion matrix
predRpart <- predict(modelRpart, validationSet)
confMatrixRpart <- confusionMatrix(predRpart, validationSet$classe)
confMatrixRpart
overallRpartOse <- 1 - as.numeric(confMatrixRpart$overall[1]) 
overallRpartOse
```
The Classification and Regression trees accuracy is **0.4953** and the out of sample error is **0.5047**

### 2. Random Forest
We will execute the Random Forest model on the training data using 250 tree and 5-fold cross validation.
```{r results="hide"}
#cluster
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)
#Random Forest train
modelRf <- train(  classe ~ ., data=trainSet,  trControl=fitControl, method='rf',ntree=250,  allowParallel = TRUE)
save(modelRf, file='./modelRf.RData')
stopCluster(cluster)
```

```{r}
#Model
modelRf
#Prediction and confusion matrix
predRf <- predict(modelRf, validationSet)						
confMatrixRf <- confusionMatrix(predRf, validationSet$classe)
confMatrixRf
overallRfOse <- 1 - as.numeric(confMatrixRf$overall[1]) 
overallRfOse
```
The Random Forest accuracy is **0.9924** and the out of sample error is **0.0076**

### 3. Stochastic Gradient Boosting
We will execute the Stochastic Gradient Boosting model on the training data using 5-fold cross validation without any others tuning parameters.
```{r results="hide"}

##cluster
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)
#Stochastic Gradient Boosting 
modelGbm <- train( classe ~ ., data=trainSet, method='gbm', trControl = fitControl)
save(modelRf, file='./modelGbm.RData')
stopCluster(cluster)
```

```{r}
#Model
modelGbm
#Prediction and confusion matrix
predGbm <- predict(modelGbm, validationSet)
confMatrixGbm <- confusionMatrix(predGbm, validationSet$classe)
confMatrixGbm
overallGbmOse <- 1 - as.numeric(confMatrixGbm$overall[1])
overallGbmOse
```
The Stochastic Gradient Boosting  accuracy is **0.9593** and the out of sample error is **0.0407**

## Model Assesment
We will compare models in terms of their resampling results. The random number seeds
were initialized to the same value prior to calling train and the same folds were used for each model.
```{r}
#accuracy comparison 
resamps <- resamples(list(rf = modelRf, gbm = modelGbm, rpart = modelRpart))
summary(resamps)
bwplot(resamps)
```
  
Based on the assessment of these 3 model both Gradient Boosting and Random Forests outperform the Classification and Regression Trees model.  
We choose Random Forests as our model because being slightly more accurate then Gradient Boosting and it's out of sample error is the lowest (0.0076).  

## Prediction on testing data
As a final step we will use Random Forest to predict a classe for each of the 20 observations in the testing data sample (‘pml-testing.csv’).
```{r}
# final test prediction
pred = predict(modelRf, testClean)
pred
```

## Conclusion
The Random Forest classification algorithms was selected, the accuracy and out of sample error are acceptable to classify new data from test data set. On the other side the accuracy is high and this it makes us feel a little bit of overfitting. We used cross validation when we builded random forests but sometimes Random Forest can lead to overfitting. Both CART and Random Forest indicates that the most differentiating variable is the roll belt.

## References 
Ugulino, W.; Cardador, D.; Vega, K.; Velloso, E.; Milidiu, R.; Fuks, H. Wearable Computing: Accelerometers' Data Classification of Body Postures and Movements. Proceedings of 21st Brazilian Symposium on Artificial Intelligence. Advances in Artificial Intelligence - SBIA 2012. In: Lecture Notes in Computer Science. , pp. 52-61. Curitiba, PR: Springer Berlin / Heidelberg, 2012. ISBN 978-3-642-34458-9. DOI: 10.1007/978-3-642-34459-6_6.

Read more: <http://groupware.les.inf.puc-rio.br/har#ixzz4pC5JdFEz> 

## Appendix
### Variable importance
Based on Random Forest Model we can identify the most important variables and eventually generate a more parsimonious model based on this variables.
```{r}
imp <- varImp(modelRf)
plot(imp, main = "25 Top Variables Importance", top = 25)
```

### Classification and Regression trees plot
```{r}
fancyRpartPlot(modelRpart$finalModel)
```

